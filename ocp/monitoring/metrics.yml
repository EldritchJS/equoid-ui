# Metrics Monitoring template.
#
# This defines all the configurable parameters and other objects that are needed to run the metrics service. This defines prometheus for
# metrics aggregation and grafana for visualization. This template can even be saved in OpenShift namespace as well so that have the
# flexibility to do any project specific customizations. Pls note wherever displayName says *** PLEASE DO NOT CHANGE THIS ***, don't touch
# that as those parameters will be referenced in other places.
#
apiVersion: v1
kind: Template
metadata:
  name: equoid-metrics-template
  namespace: equoid
  annotations:
    description: This template defines objects that are required to spin up metrics monitoring pod
    tags: prometheus, alertmanager, grafana, equoid-metrics  ,ephemeral
    openshift.io/display-name: equoid-metrics-monitoring-template
    openshift.io/long-description: "This template provides objects that are required to spin up a metrics monitoring pod. It contains
    prometheus for metrics aggregation, and grafana for visualization .The database is not stored on persistent storage, so any restart of the service will result in all data being lost."
    openshift.io/provider-display-name: Equoid-OpenShift
labels:
  createdBy: radanalytics.io
parameters:
  -
    name: PT_APP_NAME
    value: equoid-prometheus
    description: Name of the prometheus application
    required: true
    displayName: Prometheus Application Name
  -
    name: AM_APP_NAME
    value: equoid-alertmanager
    description: Name of the alert manager application
    required: true
    displayName: Alert Manager Application Name
  -
    name: GF_APP_NAME
    value: equoid-grafana
    description: Name of the grafana application
    required: true
    displayName: Grafana Application Name
  -
    name: VOLUME_CAPACITY
    displayName: Volume Capacity
    description: Volume space available for data, e.g. 512Mi, 2Gi.
    value: 1Gi
    required: true
objects:
  -
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: metrics-config
    #common configuration shared between all applications
    data:
      prometheus.conf: |-
        # Sample global config for monitoring Equoid applications
        global:
          scrape_interval:     15s # By default, scrape targets every 15 seconds.
          evaluation_interval: 15s # By default, scrape targets every 15 seconds.
          external_labels:
              monitor: 'equoid'

        # Alertmanager configuration
        alerting:
          alertmanagers:
          - static_configs:
            - targets:
               - equoid-alertmanager:9093

        # Load and evaluate rules in this file every 'evaluation_interval' seconds.
        rule_files:
          - "alert.rules"
          # - "first.rules"
          # - "second.rules"

        # A scrape configuration containing exactly one endpoint to scrape:
        # Here it's Prometheus itself.
        scrape_configs:
          # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
          - job_name: 'prometheus'

            # Override the global default and scrape targets from this job every 5 seconds.
            scrape_interval: 5s

            # scheme defaults to 'http'.
            metrics_path: /prometheusMetrics
            static_configs:
              - targets:
                  - equoid:8080
          # This is to scrape metrics of all the pods, containers, services from the openshift cluster
          # it requires the service account with correct rights, otherwise it fails with Failed to list *v1.Node: User \"system:serviceaccount:myproject:default\" cannot list nodes at the cluster scope: User \"system:serviceaccount:myproject:default\" cannot list all nodes in the cluster (get nodes)"
          # - job_name: 'openshift-nodes'
          #   tls_config:
          #     ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          #   bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          #   kubernetes_sd_configs:
          #     - role: node
          #   relabel_configs:
          #     - action: labelmap
          #       regex: __meta_openshift_node_label_(.+)
          #   metric_relabel_configs:
          #     - action: replace
          #       source_labels: [id]
          #       regex: '^/machine\.slice/machine-rkt\\x2d([^\\]+)\\.+/([^/]+)\.service$'
          #       target_label: rkt_container_name
          #       replacement: '${2}-${1}'
          #     - action: replace
          #       source_labels: [id]
          #       regex: '^/system\.slice/(.+)\.service$'
          #       target_label: systemd_service_name
          #       replacement: '${1}'
      alertmanager.conf: |-
        route:
          receiver: 'webhook'
          group_by: ['alertname']
          group_wait:      10s
          group_interval:  1m
          repeat_interval: 2h

        receivers:
        - name: 'webhook'
          webhook_configs:
          # For testing, create a sample alerting webhook at http://webhook.site/
          - url: 'http://webhook.site/5320c72c-003d-4806-a4d6-331e94a85f31'
            send_resolved: true
      alertrules.conf: |-
        groups:
        - name: equoid
          rules:
          - alert: InstanceDown
            expr: up == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              description: "stuff's happening with {{ $labels.service }}"
  -
    apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: ${PT_APP_NAME}
      labels:
        app: ${PT_APP_NAME}
    spec:
      triggers:
        -
          type: ConfigChange
      replicas: 1
      selector:
        app: ${PT_APP_NAME}
      template:
        metadata:
          labels:
            app: ${PT_APP_NAME}
        spec:
          volumes:
          - name: ${PT_APP_NAME}-data
            emptyDir: {}
          - name: ${PT_APP_NAME}-config
            configMap:
              name: metrics-config
              items:
              - key: prometheus.conf
                path: prometheus.yml
              - key: alertrules.conf
                path: alert.rules
          containers:
          - name: ${PT_APP_NAME}
            image: openshift/prometheus:v2.1.0
            args:
              - --storage.tsdb.retention=6h
              - --config.file=/etc/prometheus/prometheus.yml
              - --web.listen-address=:9090
            ports:
            - containerPort: 9090
              name: tcp
              protocol: TCP
            volumeMounts:
            - name: ${PT_APP_NAME}-data
              mountPath: /prometheus
            - name: ${PT_APP_NAME}-config
              mountPath: /etc/prometheus
            resources:
            imagePullPolicy: IfNotPresent
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
  -
    apiVersion: v1
    kind: Service
    metadata:
      name: ${PT_APP_NAME}
      labels:
        app: ${PT_APP_NAME}
    spec:
      ports:
        -
          name: tcp
          port: 9090
          protocol: TCP
          targetPort: 9090
      selector:
        app: ${PT_APP_NAME}
  -
    apiVersion: v1
    kind: Route
    metadata:
      name: ${PT_APP_NAME}
      labels:
        app: ${PT_APP_NAME}
    spec:
      to:
        kind: Service
        name: ${PT_APP_NAME}
        weight: "100"
      port:
        targetPort: "tcp"
      wildcardPolicy: None
  -
    apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: ${AM_APP_NAME}
      labels:
        app: ${AM_APP_NAME}
    spec:
      triggers:
        -
          type: ConfigChange
      replicas: 1
      selector:
        app: ${AM_APP_NAME}
      template:
        metadata:
          labels:
            app: ${AM_APP_NAME}
        spec:
          volumes:
          - name: ${AM_APP_NAME}-data
            emptyDir: {}
          - name: ${AM_APP_NAME}-config
            configMap:
              name: metrics-config
              items:
              - key: alertmanager.conf
                path: config.yml
          containers:
          - name: ${AM_APP_NAME}
            image: openshift/prometheus-alertmanager:v0.14.0
            ports:
            - containerPort: 9093
              name: tcp
            volumeMounts:
            - name: ${AM_APP_NAME}-config
              mountPath: /etc/alertmanager
            - name: ${AM_APP_NAME}-data
              mountPath: /alertmanager
            resources:
            imagePullPolicy: IfNotPresent
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
  -
    apiVersion: v1
    kind: Service
    metadata:
      name: ${AM_APP_NAME}
      labels:
        app: ${AM_APP_NAME}
    spec:
      ports:
        -
          name: tcp
          port: 9093
          targetPort: 9093
      selector:
        app: ${AM_APP_NAME}
  -
    apiVersion: v1
    kind: Route
    metadata:
      name: ${AM_APP_NAME}
      labels:
        app: ${AM_APP_NAME}
    spec:
      to:
        kind: Service
        name: ${AM_APP_NAME}
        weight: "100"
      port:
        targetPort: "tcp"
      wildcardPolicy: None
  -
    apiVersion: v1
    kind: DeploymentConfig
    metadata:
      name: ${GF_APP_NAME}
      labels:
        app: ${GF_APP_NAME}
    spec:
      triggers:
        -
          type: ConfigChange
      replicas: 1
      selector:
        app: ${GF_APP_NAME}
      template:
        metadata:
          labels:
            app: ${GF_APP_NAME}
        spec:
          volumes:
          - name: ${GF_APP_NAME}-data
            emptyDir: {}
          containers:
          - name: ${GF_APP_NAME}
            image: wkulhanek/grafana:4.6.3
            env:
            - name: GF_SECURITY_ADMIN_PASSWORD
              value: equoid
            - name: GF_USERS_ALLOW_SIGN_UP
              value: "false"
            ports:
            - containerPort: 3000
              name: http
              protocol: TCP
            volumeMounts:
            - name: ${GF_APP_NAME}-data
              mountPath: /var/lib/grafana
            resources:
            imagePullPolicy: IfNotPresent
          restartPolicy: Always
          terminationGracePeriodSeconds: 30
  -
    apiVersion: v1
    kind: Service
    metadata:
      name: ${GF_APP_NAME}
      labels:
        app: ${GF_APP_NAME}
    spec:
      ports:
        -
          name: http
          port: 3000
          protocol: TCP
          targetPort: 3000
      selector:
        app: ${GF_APP_NAME}
  -
    apiVersion: v1
    kind: Route
    metadata:
      name: ${GF_APP_NAME}
      labels:
        app: ${GF_APP_NAME}
    spec:
      to:
        kind: Service
        name: ${GF_APP_NAME}
        weight: "100"
      port:
        targetPort: "http"
      wildcardPolicy: None
